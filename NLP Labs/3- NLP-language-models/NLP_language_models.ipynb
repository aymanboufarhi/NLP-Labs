{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpOjwU967vicslBponmt15",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aymanboufarhi/NLP-language-models/blob/main/NLP_language_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab** : Get familiar with NLP language models using Sklearn library"
      ],
      "metadata": {
        "id": "0vcs1yZpgHPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 1 :**"
      ],
      "metadata": {
        "id": "6DCvrOz8giPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language Modeling / Regression :"
      ],
      "metadata": {
        "id": "zZD9NP-_gjOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset : https://github.com/dbbrandt/short_answer_granding_capstone_project/blob/master/data/sag/answers.csv"
      ],
      "metadata": {
        "id": "czS3QydSguBY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the dataset into GoogleDrive :"
      ],
      "metadata": {
        "id": "LkHbwmJ7lYBe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "TMkVFdr1ev-m",
        "outputId": "17d68ede-5e55-4005-800a-4dc869656283"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ae3b356a-3977-4f3a-89be-aff343ecd1aa\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ae3b356a-3977-4f3a-89be-aff343ecd1aa\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving answers.csv to answers.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing NLP pipeline : (Tokenization stemming lemmatization, Stop words )"
      ],
      "metadata": {
        "id": "gjm6SgvTl5TP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('answers.csv')\n",
        "\n",
        "# Initialize NLP tools\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "ps = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stop words\n",
        "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    # Stemming and Lemmatization\n",
        "    tokens = [ps.stem(word) for word in tokens]\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return tokens\n",
        "\n",
        "# Apply preprocessing\n",
        "df['processed_answer'] = df['answer'].apply(preprocess)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rKjr05m0l3yq",
        "outputId": "1a43c5b0-8189-4279-f601-198ed4319a4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode Data Vectors :"
      ],
      "metadata": {
        "id": "SbLHKTUJzIxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "# Bag of Words\n",
        "vectorizer = CountVectorizer()\n",
        "X_bow = vectorizer.fit_transform(df['answer'])\n",
        "\n",
        "# TF-IDF\n",
        "tfidf = TfidfVectorizer()\n",
        "X_tfidf = tfidf.fit_transform(df['answer'])\n",
        "\n",
        "# Word2Vec\n",
        "sentences = df['processed_answer'].tolist()\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)  # CBOW\n",
        "word2vec_model_sg = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)  # Skip Gram\n",
        "\n",
        "# Transform answers into word vectors\n",
        "def vectorize_text(text, model):\n",
        "    vector = []\n",
        "    for word in text:\n",
        "        if word in model.wv:\n",
        "            vector.append(model.wv[word])\n",
        "    if len(vector) > 0:\n",
        "        return np.mean(vector, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "df['w2v'] = df['processed_answer'].apply(lambda x: vectorize_text(x, word2vec_model))\n",
        "df['w2v_sg'] = df['processed_answer'].apply(lambda x: vectorize_text(x, word2vec_model_sg))\n",
        "\n",
        "# Ensure all vectors are of the same length and not empty\n",
        "df = df[df['w2v'].apply(lambda x: len(x) == 100)]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "s3drsIolnNVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train models by using SVR, Naive Bayes, Linear Regression , Decision Tree Algorithms :"
      ],
      "metadata": {
        "id": "tsbzOzCUzaKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Prepare data\n",
        "X = np.array(df['w2v'].tolist())\n",
        "y = df['score']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# SVR\n",
        "svr = SVR()\n",
        "svr.fit(X_train, y_train)\n",
        "y_pred_svr = svr.predict(X_test)\n",
        "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
        "\n",
        "# Linear Regression\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
        "\n",
        "# Decision Tree Regressor\n",
        "dt = DecisionTreeRegressor()\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "\n",
        "# Discretize the 'score' column into 5 bins so we can use Gaussian Naive Bayes\n",
        "discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n",
        "y_discrete = discretizer.fit_transform(df[['score']]).ravel()\n",
        "\n",
        "# Add the discretized scores to the dataframe\n",
        "df['score_discrete'] = y_discrete\n",
        "\n",
        "# Prepare data\n",
        "X = np.array(df['w2v'].tolist())\n",
        "y_discrete = df['score_discrete']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_discrete, test_size=0.2, random_state=42)\n",
        "\n",
        "# Naive Bayes\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "y_pred_nb = nb.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "print(\"Naive Bayes Accuracy:\", accuracy_nb)\n",
        "\n",
        "# Print MSE for each model\n",
        "print(\"SVR MSE:\", mse_svr)\n",
        "print(\"Linear Regression MSE:\", mse_lr)\n",
        "print(\"Decision Tree MSE:\", mse_dt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn1buBvCzN8-",
        "outputId": "65fa66ce-5b3d-4261-b42f-82f5b209c773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Accuracy: 0.5950920245398773\n",
            "SVR MSE: 1.7126606119228087\n",
            "Linear Regression MSE: 1.140539987001417\n",
            "Decision Tree MSE: 2.016559304703476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate Models :"
      ],
      "metadata": {
        "id": "4ll8aitY5Liw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Evaluation\n",
        "rmse_svr = mse_svr ** 0.5\n",
        "rmse_lr = mse_lr ** 0.5\n",
        "rmse_dt = mse_dt ** 0.5\n",
        "\n",
        "print(f\"SVR MSE: {mse_svr}, RMSE: {rmse_svr}\")\n",
        "print(f\"Linear Regression MSE: {mse_lr}, RMSE: {rmse_lr}\")\n",
        "print(f\"Decision Tree MSE: {mse_dt}, RMSE: {rmse_dt}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pu5oP4AzgDF",
        "outputId": "280a2671-c0e9-4120-b3f6-7611c4efc1df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVR MSE: 1.7126606119228087, RMSE: 1.3086865980527227\n",
            "Linear Regression MSE: 1.140539987001417, RMSE: 1.0679606673475466\n",
            "Decision Tree MSE: 2.016559304703476, RMSE: 1.420056092097589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpret the Obtained Results :"
      ],
      "metadata": {
        "id": "ewUd0dPs5yds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* SVR (Support Vector Regression) :\n",
        "\n",
        "   - MSE : 1.7126606119228087\n",
        "   - RMSE : 1.3086865980527227\n",
        "\n",
        "\n",
        "* Linear Regression :\n",
        "\n",
        "   - MSE : 1.140539987001417\n",
        "   - RMSE : 1.0679606673475466\n",
        "\n",
        "* Decision Tree Regressor :\n",
        "\n",
        "   - MSE : 2.016559304703476\n",
        "   - RMSE : 1.420056092097589\n",
        "\n",
        "## Interpretation of Results\n",
        "* Mean Squared Error (MSE) :\n",
        "   - MSE measures the average of the squares of the errorsâ€”that is, the average squared difference between the estimated values and the actual value.\n",
        "   - A lower MSE indicates a better fit.\n",
        "\n",
        "* Root Mean Squared Error (RMSE) :\n",
        "   - RMSE is the square root of MSE and provides a measure of the magnitude of the error.\n",
        "   - Like MSE, a lower RMSE indicates better model performance.\n",
        "   \n",
        "## Analysis\n",
        "\n",
        "* Linear Regression :\n",
        "\n",
        "   - Achieves the lowest MSE (1.140539987001417) and RMSE (1.0679606673475466), indicating it has the best performance among the three models.\n",
        "   - This suggests that a linear relationship between the features and the target variable fits the data well.\n",
        "\n",
        "* SVR (Support Vector Regression):\n",
        "\n",
        "   - Has a higher MSE (1.7126606119228087) and RMSE (1.3086865980527227) compared to Linear Regression.\n",
        "   - SVR might not be capturing the underlying patterns in the data as effectively as Linear Regression for this specific task.\n",
        "\n",
        "* Decision Tree Regressor:\n",
        "\n",
        "   - Exhibits the highest MSE (2.016559304703476) and RMSE (1.420056092097589) among the models.\n",
        "   - Decision Trees can overfit the training data, leading to poorer generalization to the test data, which might be the case here.\n",
        "\n",
        "## Conclusion\n",
        "Based on the MSE and RMSE metrics, Linear Regression is the best-performing model for this dataset. It indicates that a linear approach captures the relationship between the features and the target variable effectively. SVR, while useful in many contexts, does not perform as well here, and Decision Tree Regressor shows signs of overfitting, leading to higher error rates."
      ],
      "metadata": {
        "id": "uqoawVyZ6Fce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language Modeling / Classification :"
      ],
      "metadata": {
        "id": "ycB08uGq7tY_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset : https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis"
      ],
      "metadata": {
        "id": "NQkLerEIONDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the dataset into GoogleDrive :"
      ],
      "metadata": {
        "id": "ApTCUsvqROjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "B2N3ilMF5Njz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "7ef9783d-a347-4bdd-951c-be7c591af247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-21288713-c448-497e-af71-2c494caa1551\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-21288713-c448-497e-af71-2c494caa1551\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving twitter_training.csv to twitter_training.csv\n",
            "Saving twitter_validation.csv to twitter_validation.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing NLP pipeline : (Tokenization stemming lemmatization, Stop words )"
      ],
      "metadata": {
        "id": "ZMw071vaRfef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Load datasets with appropriate column names\n",
        "column_names = ['id', 'entity', 'sentiment', 'text']\n",
        "df_train = pd.read_csv('twitter_training.csv', names=column_names, header=None, encoding='latin1')\n",
        "df_val = pd.read_csv('twitter_validation.csv', names=column_names, header=None, encoding='latin1')\n",
        "\n",
        "# Combine training and validation datasets for preprocessing\n",
        "df = pd.concat([df_train, df_val])\n",
        "\n",
        "# Initialize NLP tools\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "ps = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    if isinstance(text, str):\n",
        "        # Tokenization\n",
        "        tokens = word_tokenize(text)\n",
        "        # Remove stop words\n",
        "        tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "        # Stemming and Lemmatization\n",
        "        tokens = [ps.stem(word) for word in tokens]\n",
        "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "        return ' '.join(tokens)\n",
        "    return \"\"\n",
        "\n",
        "# Apply preprocessing\n",
        "df['processed_text'] = df['text'].apply(preprocess)\n",
        "\n",
        "# Separate back into training and validation sets\n",
        "df_train = df.iloc[:len(df_train)]\n",
        "df_val = df.iloc[len(df_train):]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "X31K3b7wRe9e",
        "outputId": "781bbf76-83db-406f-fa7e-130d6420099d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode Data Vectors :"
      ],
      "metadata": {
        "id": "RflmaiWBSdEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "# Bag of Words\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_bow = vectorizer.fit_transform(df_train['processed_text'])\n",
        "X_val_bow = vectorizer.transform(df_val['processed_text'])\n",
        "\n",
        "# TF-IDF\n",
        "tfidf = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf.fit_transform(df_train['processed_text'])\n",
        "X_val_tfidf = tfidf.transform(df_val['processed_text'])\n",
        "\n",
        "# Word2Vec\n",
        "sentences = df['processed_text'].apply(word_tokenize).tolist()\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)  # CBOW\n",
        "word2vec_model_sg = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)  # Skip Gram\n",
        "\n",
        "# Transform tweets into word vectors\n",
        "def vectorize_text(text, model):\n",
        "    vector = [model.wv[word] for word in text if word in model.wv]\n",
        "    return np.mean(vector, axis=0) if vector else np.zeros(model.vector_size)\n",
        "\n",
        "# Use .loc to avoid SettingWithCopyWarning\n",
        "df_train.loc[:, 'w2v'] = df_train['processed_text'].apply(lambda x: vectorize_text(word_tokenize(x), word2vec_model))\n",
        "df_val.loc[:, 'w2v'] = df_val['processed_text'].apply(lambda x: vectorize_text(word_tokenize(x), word2vec_model))"
      ],
      "metadata": {
        "id": "pLbRRWRKSeYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Models :"
      ],
      "metadata": {
        "id": "I1JUPQTQSgAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "# Prepare data\n",
        "X_train = np.array(df_train['w2v'].tolist())\n",
        "X_val = np.array(df_val['w2v'].tolist())\n",
        "y_train = df_train['sentiment']\n",
        "y_val = df_val['sentiment']\n",
        "\n",
        "# SVM\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred_svm = svm.predict(X_val)\n",
        "\n",
        "# Naive Bayes\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "y_pred_nb = nb.predict(X_val)\n",
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred_lr = lr.predict(X_val)\n",
        "\n",
        "# AdaBoost\n",
        "ab = AdaBoostClassifier()\n",
        "ab.fit(X_train, y_train)\n",
        "y_pred_ab = ab.predict(X_val)"
      ],
      "metadata": {
        "id": "TtEoBwjWShwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate Models :"
      ],
      "metadata": {
        "id": "g4qBfebkZtdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM Evaluation\n",
        "svm_accuracy = accuracy_score(y_val, y_pred_svm)\n",
        "svm_f1 = f1_score(y_val, y_pred_svm, average='weighted')\n",
        "print(\"SVM Accuracy:\", svm_accuracy)\n",
        "print(\"SVM F1 Score:\", svm_f1)\n",
        "print(\"\\nSVM Classification Report:\")\n",
        "print(classification_report(y_val, y_pred_svm))\n",
        "\n",
        "# Naive Bayes Evaluation\n",
        "nb_accuracy = accuracy_score(y_val, y_pred_nb)\n",
        "nb_f1 = f1_score(y_val, y_pred_nb, average='weighted')\n",
        "print(\"Naive Bayes Accuracy:\", nb_accuracy)\n",
        "print(\"Naive Bayes F1 Score:\", nb_f1)\n",
        "print(\"\\nNaive Bayes Classification Report:\")\n",
        "print(classification_report(y_val, y_pred_nb))\n",
        "\n",
        "# Logistic Regression Evaluation\n",
        "lr_accuracy = accuracy_score(y_val, y_pred_lr)\n",
        "lr_f1 = f1_score(y_val, y_pred_lr, average='weighted')\n",
        "print(\"Logistic Regression Accuracy:\", lr_accuracy)\n",
        "print(\"Logistic Regression F1 Score:\", lr_f1)\n",
        "print(\"\\nLogistic Regression Classification Report:\")\n",
        "print(classification_report(y_val, y_pred_lr))\n",
        "\n",
        "# AdaBoost Evaluation\n",
        "ab_accuracy = accuracy_score(y_val, y_pred_ab)\n",
        "ab_f1 = f1_score(y_val, y_pred_ab, average='weighted')\n",
        "print(\"AdaBoost Accuracy:\", ab_accuracy)\n",
        "print(\"AdaBoost F1 Score:\", ab_f1)\n",
        "print(\"\\nAdaBoost Classification Report:\")\n",
        "print(classification_report(y_val, y_pred_ab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0hgPbBMVEQu",
        "outputId": "04b864fc-161e-4d27-f257-a1bd437ec6ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 0.565\n",
            "SVM F1 Score: 0.54482376414488\n",
            "\n",
            "SVM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Irrelevant       0.47      0.27      0.35       172\n",
            "    Negative       0.58      0.77      0.66       266\n",
            "     Neutral       0.63      0.38      0.48       285\n",
            "    Positive       0.54      0.73      0.62       277\n",
            "\n",
            "    accuracy                           0.56      1000\n",
            "   macro avg       0.56      0.54      0.53      1000\n",
            "weighted avg       0.57      0.56      0.54      1000\n",
            "\n",
            "Naive Bayes Accuracy: 0.452\n",
            "Naive Bayes F1 Score: 0.4540254091768486\n",
            "\n",
            "Naive Bayes Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Irrelevant       0.26      0.44      0.33       172\n",
            "    Negative       0.53      0.64      0.58       266\n",
            "     Neutral       0.51      0.41      0.46       285\n",
            "    Positive       0.58      0.32      0.41       277\n",
            "\n",
            "    accuracy                           0.45      1000\n",
            "   macro avg       0.47      0.45      0.44      1000\n",
            "weighted avg       0.49      0.45      0.45      1000\n",
            "\n",
            "Logistic Regression Accuracy: 0.518\n",
            "Logistic Regression F1 Score: 0.4927583728315884\n",
            "\n",
            "Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Irrelevant       0.43      0.12      0.18       172\n",
            "    Negative       0.49      0.73      0.59       266\n",
            "     Neutral       0.52      0.54      0.53       285\n",
            "    Positive       0.57      0.55      0.56       277\n",
            "\n",
            "    accuracy                           0.52      1000\n",
            "   macro avg       0.50      0.48      0.46      1000\n",
            "weighted avg       0.51      0.52      0.49      1000\n",
            "\n",
            "AdaBoost Accuracy: 0.501\n",
            "AdaBoost F1 Score: 0.4751475366943781\n",
            "\n",
            "AdaBoost Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Irrelevant       0.48      0.12      0.19       172\n",
            "    Negative       0.47      0.73      0.57       266\n",
            "     Neutral       0.53      0.47      0.50       285\n",
            "    Positive       0.52      0.55      0.53       277\n",
            "\n",
            "    accuracy                           0.50      1000\n",
            "   macro avg       0.50      0.47      0.45      1000\n",
            "weighted avg       0.50      0.50      0.48      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpret the Obtained Results :"
      ],
      "metadata": {
        "id": "lUi65s10dllb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Support Vector Machine (SVM) :\n",
        "\n",
        "   - Accuracy : 56.5%\n",
        "   - F1 Score : 54.48%\n",
        "   - Precision : Ranges from 47% to 63% for different classes.\n",
        "   - Recall : Ranges from 27% to 77% for different classes.\n",
        "   - Interpretation : SVM performs moderately well with an accuracy slightly above chance. However, it seems to struggle with precision and recall, especially for the \"Irrelevant\" and \"Neutral\" classes.\n",
        "\n",
        "* Naive Bayes :\n",
        "\n",
        "   - Accuracy : 45.2%\n",
        "   - F1 Score : 45.40%\n",
        "   - Precision : Ranges from 26% to 58% for different classes.\n",
        "   - Recall : Ranges from 32% to 64% for different classes.\n",
        "   - Interpretation : Naive Bayes shows lower performance compared to SVM, with lower accuracy and F1 score. It tends to have varied precision and recall across different classes, indicating a more generalized performance.\n",
        "\n",
        "* Logistic Regression:\n",
        "\n",
        "   - Accuracy : 51.8%\n",
        "   - F1 Score : 49.28%\n",
        "   - Precision : Ranges from 43% to 57% for different classes.\n",
        "   - Recall : Ranges from 12% to 73% for different classes.\n",
        "   - Interpretation : Logistic Regression performs slightly better than Naive Bayes but worse than SVM. It shows decent precision but struggles with recall, particularly evident in the \"Irrelevant\" class.\n",
        "\n",
        "* AdaBoost:\n",
        "\n",
        "   - Accuracy : 50.1%\n",
        "   - F1 Score : 47.51%\n",
        "   - Precision : Ranges from 47% to 53% for different classes.\n",
        "   - Recall : Ranges from 12% to 73% for different classes.\n",
        "   - Interpretation : AdaBoost's performance is similar to Logistic Regression, with slightly lower accuracy and F1 score. It also exhibits a struggle with recall, especially for the \"Irrelevant\" class.\n",
        "\n",
        "Overall, SVM outperforms the other classifiers, followed by Logistic Regression and AdaBoost, while Naive Bayes exhibits the lowest performance in terms of accuracy and F1 score. All models encounter challenges in correctly classifying instances, particularly in certain classes, as indicated by variations in precision and recall across classes."
      ],
      "metadata": {
        "id": "YLCnL5kTcaqA"
      }
    }
  ]
}